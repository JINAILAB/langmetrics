{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCQ (Multiple Choice Question) í‰ê°€ íŠœí† ë¦¬ì–¼\n",
    "\n",
    "## MCQDataset\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Huggingfaceì˜ ê°ê´€ì‹ datasetì„ ë¶ˆëŸ¬ì™€ì„œ í‰ê°€ í›„ ì¬ì—…ë¡œë“œí•˜ëŠ” ê³¼ì •ê¹Œì§€ ê²½í—˜í•´ë³¼ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ë¨¼ì € HuggingFace Hubì—ì„œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmetrics.llmdataset import MCQDataset\n",
    "from langmetrics.llmtestcase import MCQTestCase\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('sickgpt/001_MedQA_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'expected_output', 'choices'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'expected_output', 'choices'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ MCQDatasetì„ ì´ìš©í•´ì„œ ë¶ˆëŸ¬ì™€ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¨¼ì € MCQTestCaseëŠ” input, choices, expected_outputì„ ê³ ì •ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ìœ„ì— Datasetì€ inputì´ questionì´ë¼ëŠ” ì—´ë¡œ ë˜ì–´ìˆë„¤ìš”. field_mapping ì¸ìë¥¼ ì´ìš©í•´ì„œ columnì„ ë§¤í•‘í•´ì£¼ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': str,\n",
       " 'choices': typing.List[str],\n",
       " 'expected_output': typing.Union[int, str],\n",
       " 'output': typing.Optional[str],\n",
       " 'reasoning': typing.Optional[str]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCQTestCase.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì‹œ ì‚¬ìš©ë²•\n",
    "field_mapping = {\n",
    "    'input': 'question',  # ë°ì´í„°ì…‹ì˜ 'question' í•„ë“œë¥¼ 'input'ìœ¼ë¡œ ë§¤í•‘\n",
    "    'expected_output': 'expected_output',\n",
    "    'choices': 'choices'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MCQDataset.from_huggingface_hub('sickgpt/001_MedQA_raw', field_mapping=field_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MCQDataset.from_huggingface_hub('sickgpt/001_MedQA_raw', field_mapping=field_mapping, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ evaluateì„ ì§„í–‰í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmetrics.llmfactory import LLMFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o',\n",
       " 'gpt-4o-mini',\n",
       " 'deepseek-v3',\n",
       " 'deepseek-reasoner',\n",
       " 'claude-3.5-sonnet',\n",
       " 'claude-3.5-haiku',\n",
       " 'naver']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLMFactory.get_model_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmetrics.llmfactory import LLMFactory\n",
    "# LLM ëª¨ë¸ ìƒì„±\n",
    "gpt_4o_mini = LLMFactory.create_llm('gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1 = LLMFactory.create_llm('deepseek-reasoner', temperature=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmetrics.metrics import MCQMetric\n",
    "metric = MCQMetric(\n",
    "    answer_model=gpt_4o_mini,\n",
    "    template_language='en',  # 'ko' ë˜ëŠ” 'en'\n",
    "    generate_template_type='reasoning'  # 'reasoning' ë˜ëŠ” 'only_answer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langmetrics.metrics import MCQMetric\n",
    "# r1_metric = MCQMetric(\n",
    "#     answer_model=deepseek_r1,\n",
    "#     template_language='en',  # 'ko' ë˜ëŠ” 'en'\n",
    "#     generate_template_type='reasoning', # 'reasoning' ë˜ëŠ” 'only_answer'\n",
    "#     verbose_mode=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asyncë¥¼ í†µí•´ì„œ ë¹ ë¥´ê²Œ ì¶”ë¡ ì„ í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCQTestCase(input='A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?', choices=['Disclose the error to the patient and put it in the operative report', 'Tell the attending that he cannot fail to disclose this mistake', 'Report the physician to the ethics committee', 'Refuse to dictate the operative report'], expected_output='B', output=None, reasoning=None)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await metric.ameasure(test_dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1_results = await r1_metric.ameasure(test_dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•½ 1200ê°œì˜ ë‹¬í•˜ëŠ” testë¥¼ ë‹¨ 30ì´ˆë§Œì— ëª¨ë‘ ì¶”ë¡ í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sum([i.score for i in results]) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1_scores = sum([i.score for i in r1_results]) / len(r1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“ ë¬¸ì œ: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?\n",
      "\n",
      "ğŸ¤” ì œì¶œí•œ ë‹µ: {\n",
      "    \"reasoning\": \"In this scenario, the resident faces an ethical dilemma involving disclosure of a medical error. It is essential to maintain transparency in medical practice for patient safety and trust. While the attending physician suggests not reporting the error, the resident has a duty to prioritize the patient's right to know about any complications that occurred during their surgery, regardless of the perceived severity. Therefore, the best course of action would be to disclose the error to the patient and include it in the operative report, as this aligns with ethical standards in medicine. So the answer is A.\",\n",
      "    \"answer\": \"A\"\n",
      "}\n",
      "ğŸ“‹ ì„ íƒì§€: ['Disclose the error to the patient and put it in the operative report', 'Tell the attending that he cannot fail to disclose this mistake', 'Report the physician to the ethics committee', 'Refuse to dictate the operative report']\n",
      "ğŸ’¡ ì •ë‹µ: B\n",
      "\n",
      "ğŸ“Š ì±„ì  ê²°ê³¼: âŒ ì˜¤ë‹µ\n",
      "\n",
      "ğŸ’­ ì¶”ë¡  ê³¼ì •:\n",
      "In this scenario, the resident faces an ethical dilemma involving disclosure of a medical error. It is essential to maintain transparency in medical practice for patient safety and trust. While the attending physician suggests not reporting the error, the resident has a duty to prioritize the patient's right to know about any complications that occurred during their surgery, regardless of the perceived severity. Therefore, the best course of action would be to disclose the error to the patient and include it in the operative report, as this aligns with ethical standards in medicine. So the answer is A.\n",
      "\n",
      "â„¹ï¸ ë©”íƒ€ë°ì´í„°: {'language': 'en', 'student_token_usage': {'completion_tokens': 124, 'prompt_tokens': 279, 'total_tokens': 403}}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"question\": [result.question for result in results],\n",
    "    \"choice\": [\", \".join(result.choice) for result in results],  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    \"predicted\": [result.predicted for result in results],\n",
    "    \"score\": [result.score for result in results],\n",
    "    \"ground_truth\": [result.ground_truth for result in results],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv('deepseek_medqa_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
